{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hxo20tkU7LHx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVoMC5Hr7Qzg",
        "outputId": "d3bf3f32-79ba-4424-c40e-143225576b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc-KrXj6YUBP",
        "outputId": "b4205e77-28c7-4777-a9bf-0e57cdf4eac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyedflib\n",
            "  Downloading pyEDFlib-0.1.38-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from pyedflib) (1.26.4)\n",
            "Downloading pyEDFlib-0.1.38-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m2.6/2.7 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyedflib\n",
            "Successfully installed pyedflib-0.1.38\n"
          ]
        }
      ],
      "source": [
        "!pip install pyedflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ONDsY-k4YT-H"
      },
      "outputs": [],
      "source": [
        "import pyedflib\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from scipy.fft import fft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pywavelets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBLMEmslmmeR",
        "outputId": "284671fb-e2e0-47e0-9d14-a5efc03c8c08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pywavelets\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from pywavelets) (1.26.4)\n",
            "Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m3.3/4.5 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pywavelets\n",
            "Successfully installed pywavelets-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyedflib\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.signal import butter, lfilter, resample\n",
        "import pywt\n",
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "Vx3-rssamVfj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQasg2XXQU_8",
        "outputId": "6fae1fda-7d8d-45e6-e959-88f6237aa91c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pyedflib\n",
        "import pywt\n",
        "\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, folder_path, labels_dict, target_length=160000, wavelet='sym2', wavelet_level=3):\n",
        "        self.folder_path = folder_path\n",
        "        self.labels_dict = labels_dict\n",
        "        self.file_list = [f for f in os.listdir(folder_path) if f.endswith(\".edf\")]\n",
        "        self.target_length = target_length\n",
        "        self.wavelet = wavelet\n",
        "        self.wavelet_level = wavelet_level\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.file_list[idx]\n",
        "        file_path = os.path.join(self.folder_path, filename)\n",
        "\n",
        "        # Load the EDF file only when needed\n",
        "        signals = self.load_edf_file_raw(file_path)\n",
        "\n",
        "        # Preprocessing the signals using wavelet transform\n",
        "        signals = self.preprocess_eeg_signal_wavelet(signals, wavelet=self.wavelet, level=self.wavelet_level)\n",
        "\n",
        "        # Padding or truncating the signals to ensure all have the same length\n",
        "        signals = self.pad_or_truncate(signals, self.target_length)\n",
        "\n",
        "        # Extracting the file number from the filename\n",
        "        file_number = filename.replace('.edf', '')\n",
        "\n",
        "        # Get the label for the current file\n",
        "        label = self.labels_dict.get(file_number, -1)  # Default to -1 if no label found\n",
        "\n",
        "        # Convert signals and label to tensors\n",
        "        signals_tensor = torch.Tensor(signals)\n",
        "        label_tensor = torch.LongTensor([label]).squeeze()\n",
        "\n",
        "        return signals_tensor, label_tensor\n",
        "\n",
        "    def load_edf_file_raw(self, file_path):\n",
        "        f = pyedflib.EdfReader(file_path)\n",
        "        n = f.signals_in_file\n",
        "        signals = np.zeros((n, f.getNSamples()[0]))  # Creating array for raw data\n",
        "\n",
        "        # Reading all channels\n",
        "        for i in range(n):\n",
        "            signals[i, :] = f.readSignal(i)\n",
        "\n",
        "        f.close()\n",
        "        return signals\n",
        "\n",
        "    def preprocess_eeg_signal_wavelet(self, raw_signal, wavelet='db4', level=3):\n",
        "        coeffs = []\n",
        "        for channel in raw_signal:\n",
        "            # Performing a multilevel wavelet decomposition for each channel\n",
        "            channel_coeffs = pywt.wavedec(channel, wavelet, level=level)\n",
        "            coeffs.append(channel_coeffs)\n",
        "\n",
        "        # Reconstruct signals from selected approximation and detail coefficients\n",
        "        reconstructed_signal = []\n",
        "        for channel_coeffs in coeffs:\n",
        "            reconstructed_channel = pywt.waverec([channel_coeffs[0]] + [None] * (len(channel_coeffs) - 1), wavelet)\n",
        "            reconstructed_signal.append(reconstructed_channel)\n",
        "\n",
        "        return np.array(reconstructed_signal)\n",
        "\n",
        "    def pad_or_truncate(self, signal, target_length):\n",
        "        if signal.shape[1] > target_length:\n",
        "            return signal[:, :target_length]\n",
        "        else:\n",
        "            padding = np.zeros((signal.shape[0], target_length - signal.shape[1]))\n",
        "            return np.concatenate((signal, padding), axis=1)\n"
      ],
      "metadata": {
        "id": "QG2uw7JrQgQ_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-cByPlefYT4f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/Harshitha/annotation.xlsx'\n",
        "labels_df = pd.read_excel(csv_path)\n",
        "\n",
        "def map_labels(row):\n",
        "    if row['dementia'] == 1:\n",
        "        return 2  # Dementia\n",
        "    elif row['mci'] == 1:\n",
        "        return 1  # MCI\n",
        "    else:\n",
        "        return 0  # Normal\n",
        "\n",
        "labels_df['Label'] = labels_df.apply(map_labels, axis=1)\n",
        "\n",
        "labels_dict = dict(zip(labels_df['serial'].astype(str).str.zfill(5), labels_df['Label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "X-eF7kt0YT1P"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/MyDrive/Harshitha/final_edf'\n",
        "\n",
        "# Creating the EEGDataset instance for lazy loading\n",
        "eeg_dataset = EEGDataset(\n",
        "    folder_path=folder_path,\n",
        "    labels_dict=labels_dict,\n",
        "    target_length=160000,\n",
        "    wavelet='sym2',\n",
        "    wavelet_level=3\n",
        ")\n",
        "\n",
        "dataset_size = len(eeg_dataset)\n",
        "test_split = 0.2\n",
        "test_size = int(dataset_size * test_split)\n",
        "train_size = dataset_size - test_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(eeg_dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BYdJLsJhYTyW"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EEG_CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EEG_CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=21, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.fc1 = None\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the CNN\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "\n",
        "        if self.fc1 is None:\n",
        "            print(f\"Flattened output size: {x.size(1)}\")\n",
        "            self.fc1 = nn.Linear(x.size(1), 128)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_model(model, train_loader, num_epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Loop over batches of data\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with autocast for mixed precision\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization with mixed precision scaling\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Calculate statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculation of epoch metrics\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "\n",
        "        # Get the current learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "              f'Loss: {epoch_loss:.4f}, '\n",
        "              f'Accuracy: {epoch_accuracy:.2f}%, '\n",
        "              f'LR: {current_lr:.6f}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "6qkt6UgsIM6X"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAsefUBlYTtA",
        "outputId": "c3af1db7-d042-49da-a1da-d54017b870c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-325be1554b1c>:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-33-325be1554b1c>:45: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened output size: 2560000\n",
            "Epoch 1/10, Loss: 1.5635, Accuracy: 29.17%, LR: 0.001000\n",
            "Epoch 2/10, Loss: 1.2136, Accuracy: 34.17%, LR: 0.001000\n",
            "Epoch 3/10, Loss: 1.0207, Accuracy: 50.83%, LR: 0.001000\n",
            "Epoch 4/10, Loss: 0.8748, Accuracy: 59.17%, LR: 0.001000\n",
            "Epoch 5/10, Loss: 0.8579, Accuracy: 60.83%, LR: 0.001000\n",
            "Epoch 6/10, Loss: 0.7755, Accuracy: 70.00%, LR: 0.001000\n",
            "Epoch 7/10, Loss: 0.7159, Accuracy: 74.17%, LR: 0.001000\n",
            "Epoch 8/10, Loss: 0.6814, Accuracy: 75.83%, LR: 0.001000\n",
            "Epoch 9/10, Loss: 0.6584, Accuracy: 74.17%, LR: 0.001000\n",
            "Epoch 10/10, Loss: 0.5544, Accuracy: 86.67%, LR: 0.001000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EEG_CNN(\n",
              "  (conv1): Conv1d(21, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (relu): ReLU()\n",
              "  (fc1): Linear(in_features=2560000, out_features=128, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "num_classes = 3  # (Normal, MCI, Dementia)\n",
        "input_sequence_length = 160000\n",
        "model = EEG_CNN(num_classes=num_classes)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_dataloader, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo37PpKXJux1",
        "outputId": "9b7bd5a8-0f68-4040-e81b-367bd9db0560"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/869.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.7 torchmetrics-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Precision, Recall, F1Score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def evaluate_model(model, dataloader, num_classes=3):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_probs.append(outputs.cpu())\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    all_probs = torch.cat(all_probs)\n",
        "\n",
        "    # Computation of precision, recall, F1-score\n",
        "    precision_value = Precision(task='multiclass', num_classes=num_classes, average='macro')(all_preds, all_labels)\n",
        "    recall_value = Recall(task='multiclass', num_classes=num_classes, average='macro')(all_preds, all_labels)\n",
        "    f1_value = F1Score(task='multiclass', num_classes=num_classes, average='macro')(all_preds, all_labels)\n",
        "\n",
        "    print(f\"Precision: {precision_value.item():.4f}\")\n",
        "    print(f\"Recall: {recall_value.item():.4f}\")\n",
        "    print(f\"F1 Score: {f1_value.item():.4f}\")\n",
        "\n",
        "# Evaluatation of the model on the test set\n",
        "evaluate_model(model, test_dataloader, num_classes=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW2vrEqrJy35",
        "outputId": "22faa76f-5a3c-478b-ff3d-60c7a4d71b68"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.3168\n",
            "Recall: 0.3611\n",
            "F1 Score: 0.3317\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}